{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the initial tests with Gemini checking what Michal did. Also what he did with GPT, then a successful fine tuning and a failing embedding ty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google.oauth2 import service_account\n",
    "import google.ai.generativelanguage as glm\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "# import genai\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "# Transformers\n",
    "# Hugging Face libraries for tokenization and model handling\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Hugging Face Datasets library for dataset management\n",
    "from datasets import Dataset\n",
    "\n",
    "# scikit-learn for label encoding and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Lora\n",
    "from peft import PeftModel, LoraConfig, LoraModel  # Use LoRA as per the new PEFT library structure\n",
    "import torch\n",
    "\n",
    "# import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Michal.classfier_google import *\n",
    "from Michal.classfier_gpt import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\RodrigoMartínezAlons\\OneDrive - Sparrow Networks GmbH\\Python\\categorization\\LLMs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check why cant I call the function in the other script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to see the buckets in our Google Cloud profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your service account JSON key file\n",
    "service_account_file = 'key/eclass-key.json'\n",
    "\n",
    "# Authenticate using the service account file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "\n",
    "client = storage.Client(credentials=credentials, project='your-project-id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all buckets in the Google Cloud project\n",
    "# buckets = client.list_buckets()\n",
    "\n",
    "# # Print the names of the buckets\n",
    "# print(\"Buckets in the project:\")\n",
    "# for bucket in buckets:\n",
    "#     print(bucket.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes for adapting the code to a new functionality. Basicly introducing the prompt as an input for the model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probability': 0.9987522959709167, 'answer': 'The product description is classified as a SPEAKER.', 'grounding': 'CLASS ID:2385, English: SPEAKER, German:Lautsprecher'}\n"
     ]
    }
   ],
   "source": [
    "# Example of a custom prompt for classification\n",
    "prompt_template = \"Classify the product description: {text}. What category does it belong to?\"\n",
    "sample_text = \"A portable Bluetooth speaker with amazing sound quality.\"\n",
    "corpus_resource_name = \"corpora/nounmodifier-classifier-xvqy7imlpxai\"\n",
    "service_account_file = \"key/eclass-key.json\"\n",
    "result = process_text_with_google(sample_text, service_account_file, corpus_resource_name, prompt_template)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cant also get the corpus of documents. Tried to extract it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error listing models: 'GenerativeServiceClient' object has no attribute 'get_corpus'\n"
     ]
    }
   ],
   "source": [
    "# Function to list models inside a corpus (retrieve corpus metadata)\n",
    "def list_models_in_corpus(generative_service_client, corpus_resource_name):\n",
    "    try:\n",
    "        corpus = generative_service_client.get_corpus(corpus_resource_name)\n",
    "        print(\"Models in the corpus:\", corpus.models)\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {e}\")\n",
    "\n",
    "# Usage example\n",
    "service_account_file = \"key/eclass-key.json\"\n",
    "corpus_resource_name = \"corpora/nounmodifier-classifier-xvqy7imlpxai\"\n",
    "generative_service_client = initialize_google_client(service_account_file)\n",
    "list_models_in_corpus(generative_service_client, corpus_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Trying Gemini-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyBI44986WtAGEYL158OtjgcApEK1qTjU8o\")\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "model.temperature = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_async_client', '_client', '_generation_config', '_get_tools_lib', '_model_name', '_prepare_request', '_safety_settings', '_system_instruction', '_tool_config', '_tools', 'cached_content', 'count_tokens', 'count_tokens_async', 'from_cached_content', 'generate_content', 'generate_content_async', 'model_name', 'start_chat', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "print(dir(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate_content in module google.generativeai.generative_models:\n",
      "\n",
      "generate_content(contents: 'content_types.ContentsType', *, generation_config: 'generation_types.GenerationConfigType | None' = None, safety_settings: 'safety_types.SafetySettingOptions | None' = None, stream: 'bool' = False, tools: 'content_types.FunctionLibraryType | None' = None, tool_config: 'content_types.ToolConfigType | None' = None, request_options: 'helper_types.RequestOptionsType | None' = None) -> 'generation_types.GenerateContentResponse' method of google.generativeai.generative_models.GenerativeModel instance\n",
      "    A multipurpose function to generate responses from the model.\n",
      "\n",
      "    This `GenerativeModel.generate_content` method can handle multimodal input, and multi-turn\n",
      "    conversations.\n",
      "\n",
      "    >>> model = genai.GenerativeModel('models/gemini-pro')\n",
      "    >>> response = model.generate_content('Tell me a story about a magic backpack')\n",
      "    >>> response.text\n",
      "\n",
      "    ### Streaming\n",
      "\n",
      "    This method supports streaming with the `stream=True`. The result has the same type as the non streaming case,\n",
      "    but you can iterate over the response chunks as they become available:\n",
      "\n",
      "    >>> response = model.generate_content('Tell me a story about a magic backpack', stream=True)\n",
      "    >>> for chunk in response:\n",
      "    ...   print(chunk.text)\n",
      "\n",
      "    ### Multi-turn\n",
      "\n",
      "    This method supports multi-turn chats but is **stateless**: the entire conversation history needs to be sent with each\n",
      "    request. This takes some manual management but gives you complete control:\n",
      "\n",
      "    >>> messages = [{'role':'user', 'parts': ['hello']}]\n",
      "    >>> response = model.generate_content(messages) # \"Hello, how can I help\"\n",
      "    >>> messages.append(response.candidates[0].content)\n",
      "    >>> messages.append({'role':'user', 'parts': ['How does quantum physics work?']})\n",
      "    >>> response = model.generate_content(messages)\n",
      "\n",
      "    For a simpler multi-turn interface see `GenerativeModel.start_chat`.\n",
      "\n",
      "    ### Input type flexibility\n",
      "\n",
      "    While the underlying API strictly expects a `list[protos.Content]` objects, this method\n",
      "    will convert the user input into the correct type. The hierarchy of types that can be\n",
      "    converted is below. Any of these objects can be passed as an equivalent `dict`.\n",
      "\n",
      "    * `Iterable[protos.Content]`\n",
      "    * `protos.Content`\n",
      "    * `Iterable[protos.Part]`\n",
      "    * `protos.Part`\n",
      "    * `str`, `Image`, or `protos.Blob`\n",
      "\n",
      "    In an `Iterable[protos.Content]` each `content` is a separate message.\n",
      "    But note that an `Iterable[protos.Part]` is taken as the parts of a single message.\n",
      "\n",
      "    Arguments:\n",
      "        contents: The contents serving as the model's prompt.\n",
      "        generation_config: Overrides for the model's generation config.\n",
      "        safety_settings: Overrides for the model's safety settings.\n",
      "        stream: If True, yield response chunks as they are generated.\n",
      "        tools: `protos.Tools` more info coming soon.\n",
      "        request_options: Options for the request.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"What do you call a boomerang that won't come back?\\n\\nA stick.\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 6,\n",
      "        \"candidates_token_count\": 18,\n",
      "        \"total_token_count\": 24\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Tell me a joke!\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.0\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(openai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the module\n",
    "import gpt_siemens as siemens\n",
    "\n",
    "# Example Query\n",
    "query = \"Are 3rh29111ha11 and 3rh29111ha13 from siemens the same product??\"\n",
    "\n",
    "# Process Query\n",
    "result = siemens.process_query(query)\n",
    "\n",
    "# Print the Result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': {'noun': 'friends', 'modifier': 'incredible'}}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sample_text = 'I have two incredible friends'\n",
    "result = process(sample_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply for the data Michal used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Noun-Modifier classes\n",
    "classes = pd.read_excel('data/SSNM_v1.xlsx', sheet_name='Sparrow', dtype='str')\n",
    "class_desc = {}\n",
    "for x in classes.to_dict('records'):\n",
    "    class_desc[int(x['ID'])-1] = {'noun':x['Noun'], 'modifier':x['Modifier'],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load DATA to be classified and select field with description, and product id field\n",
    "\n",
    "description_field = 'LongDescription'\n",
    "product_id_field = 'MATNR'\n",
    "csv_file_name = 'data/initialProductImport2.csv'\n",
    "\n",
    "data = pd.read_csv(csv_file_name).fillna('')\n",
    "data = data[[product_id_field,description_field]]\n",
    "data = data.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = []\n",
    "id_processed = [x['id'] for x in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MATNR': 2164108,\n",
       "  'LongDescription': '    REER 16 entré Mosaic MI16 ITFSICURE00046  GIARDINA FINISHING s.r.l REER 16 entré Mosaic MI16 ITFSICURE00046'},\n",
       " {'MATNR': 2083096,\n",
       "  'LongDescription': '    SIEMENS CONVERTER  MICROMASTER 6ES6440-2  SIEMENS AG MICROMA.FREQU.RICHT. 6ES6440-2AD32-2DA1'},\n",
       " {'MATNR': 2127663,\n",
       "  'LongDescription': '    Float,special cover plate RHS\\xa0404.5*970*  Advance metal design special cover plate RHS\\xa0405-06 522/23'},\n",
       " {'MATNR': 401154,\n",
       "  'LongDescription': '    PUMP PLP-30   RUBBER BEARING 513171-101-  SULZER PUMPS FINLAND OY PUMPUN PLP-30   KUMILAAKERI 513171-101-'},\n",
       " {'MATNR': 2178031,\n",
       "  'LongDescription': '    FILTER-REGULATOR UNIT ATT.1\"\" AW60-F10D-B  BOTTERO S.p.A. FILTER-REGULATOR UNIT ATT.1\" AW60-F10D-B'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:03<00:12,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mapping'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:04<00:06,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mapping'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:08<00:06,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mapping'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:13<00:03,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mapping'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'mapping'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm(data[:5]):\n",
    "    \n",
    "    if item[product_id_field] in id_processed:\n",
    "        continue\n",
    "    try:\n",
    "        it_processed = process(item[description_field])\n",
    "        it={}\n",
    "        for k,v in it_processed['result'].items():\n",
    "            it[k]=v if v else ''\n",
    "            \n",
    "        it['id']=item[product_id_field]\n",
    "        try:\n",
    "            mapping = it_processed['mapping']\n",
    "            #sort by score desc\n",
    "            mapping = sorted(mapping, key=lambda x: x[1], reverse=True)\n",
    "            mapping = mapping[0]\n",
    "            it['mapped_noun'] = class_desc[mapping[0]]['noun']\n",
    "            it['mapped_modifier'] = class_desc[mapping[0]]['modifier']\n",
    "            it['score'] = mapping[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    processed.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'noun': '[REER 16 entré Mosaic MI16 ITFSICURE00046  GIARDINA FINISHING s.r.l REER 16 entré Mosaic MI16 ITFSICURE00046]',\n",
       "  'modifier': '[]',\n",
       "  'id': 2164108},\n",
       " {'noun': 'SIEMENS CONVERTER',\n",
       "  'modifier': 'MICROMASTER\\nnoun=SIEMENS AG,',\n",
       "  'id': 2083096},\n",
       " {'noun': '[Float]',\n",
       "  'modifier': '[]\\nnoun=[special cover plate RHS],',\n",
       "  'id': 2127663},\n",
       " {'noun': 'PUMP PLP-30',\n",
       "  'modifier': 'noun=RUBBER BEARING 513171-101-,',\n",
       "  'id': 401154},\n",
       " {'noun': '[FILTER-REGULATOR UNIT]',\n",
       "  'modifier': '[ATT.1\"\" AW60-F10D-B BOTTERO S.p.A]',\n",
       "  'id': 2178031}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# #Save the result\n",
    "# output_file_name = csv_file_name.split('.')[0] + '_processed.json'\n",
    "# with open(output_file_name+'.json','w') as f:\n",
    "#     f.write(json.dumps(processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Fine tuning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only those categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('catalog_nouns.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Id</th>\n",
       "      <th>Quality Status</th>\n",
       "      <th>Manufacturer Name</th>\n",
       "      <th>Manufacturer PID</th>\n",
       "      <th>model</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Long Description</th>\n",
       "      <th>Additional description</th>\n",
       "      <th>score</th>\n",
       "      <th>Product family</th>\n",
       "      <th>noun</th>\n",
       "      <th>norm</th>\n",
       "      <th>cleaned_noun</th>\n",
       "      <th>category_noun</th>\n",
       "      <th>matched_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61753559</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aderleitung flex H05V-K 1mm² WS</td>\n",
       "      <td>Aderleitung flex H05V-K 1mm² WS; ;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aderleitung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aderleitung</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61635835</td>\n",
       "      <td>SPARROW_APPROVED</td>\n",
       "      <td>FESTO</td>\n",
       "      <td>541279</td>\n",
       "      <td>541279</td>\n",
       "      <td>Einschaltventil</td>\n",
       "      <td>Valve Ms6 Em1 3/8 541279; FESTO CTE 541279-148...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Einschaltventil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>einschaltventil</td>\n",
       "      <td>Valves, Actuator, Fittings</td>\n",
       "      <td>einschaltventil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Id      Quality Status Manufacturer Name Manufacturer PID   model  \\\n",
       "0    61753559  SPARROW_PROCESSING               NaN              NaN     NaN   \n",
       "1    61635835    SPARROW_APPROVED             FESTO           541279  541279   \n",
       "\n",
       "                 Short Description  \\\n",
       "0  Aderleitung flex H05V-K 1mm² WS   \n",
       "1                  Einschaltventil   \n",
       "\n",
       "                                    Long Description Additional description  \\\n",
       "0                Aderleitung flex H05V-K 1mm² WS; ;                     NaN   \n",
       "1  Valve Ms6 Em1 3/8 541279; FESTO CTE 541279-148...                    NaN   \n",
       "\n",
       "  score Product family             noun norm     cleaned_noun  \\\n",
       "0   NaN            NaN      Aderleitung  NaN      aderleitung   \n",
       "1   NaN            NaN  Einschaltventil  NaN  einschaltventil   \n",
       "\n",
       "                category_noun     matched_noun  \n",
       "0                        None             None  \n",
       "1  Valves, Actuator, Fittings  einschaltventil  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_filter = [\n",
    "    \"Fasteners\", \n",
    "    \"Gas, water and sewage installation\", \n",
    "    \"Electrical Installation Materials, device\", \n",
    "    \"Piping Materials\", \n",
    "    \"Valves, Actuator, Fittings\", \n",
    "    \"Facility consumables\",\n",
    "    \"Equipment OEM Spare Parts\"]\n",
    "\n",
    "classify = df[df['category_noun'].isin(categories_to_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify=classify[['cleaned_noun', 'category_noun']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_noun</th>\n",
       "      <th>category_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>einschaltventil</td>\n",
       "      <td>Valves, Actuator, Fittings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>springs</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filling wheel</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coupler</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oring</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cleaned_noun               category_noun\n",
       "1  einschaltventil  Valves, Actuator, Fittings\n",
       "2          springs                   Fasteners\n",
       "3    filling wheel                   Fasteners\n",
       "5          coupler                   Fasteners\n",
       "8            oring                   Fasteners"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a list of tuples (or named tuples)\n",
    "tuple_list = [tuple(x) for x in classify.itertuples(index=False, name=None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('einschaltventil', 'Valves, Actuator, Fittings'),\n",
       " ('springs', 'Fasteners'),\n",
       " ('filling wheel', 'Fasteners'),\n",
       " ('coupler', 'Fasteners'),\n",
       " ('oring', 'Fasteners'),\n",
       " ('packing', 'Fasteners'),\n",
       " ('clamp', 'Fasteners'),\n",
       " ('cap', 'Fasteners'),\n",
       " ('wellendichtring', 'Fasteners'),\n",
       " ('snap wrap rubber spider', 'Piping Materials')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('class.jsonl', 'w') as file:\n",
    "#     for item in tuple_list:\n",
    "#         # Create a dictionary for each tuple (prompt and completion)\n",
    "#         json_obj = {\"prompt\": item[0], \"completion\": item[1]}\n",
    "#         # Write the dictionary as a JSON string followed by a newline\n",
    "#         file.write(json.dumps(json_obj) + '\\n')\n",
    "\n",
    "# print(\"Data successfully written to class.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key\n",
    "\n",
    "# Step 1: Upload the training and validation datasets\n",
    "with open(\"fine_tun_json/class_prepared_train.jsonl\", \"rb\") as train_file:\n",
    "    train_response = openai.File.create(file=train_file, purpose='fine-tune')\n",
    "\n",
    "with open(\"fine_tun_json/class_prepared_valid.jsonl\", \"rb\") as valid_file:\n",
    "    valid_response = openai.File.create(file=valid_file, purpose='fine-tune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.files.create(file=open(\"class_prepared_valid.jsonl\", \"rb\"), purpose='fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='fine_tun_json/class_prepared_train.jsonl'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = openai.File.create(file=open(\"class_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "# valid_file = openai.File.create(file=open(\"class_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=openai.file_from_path(\"class_prepared_valid.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually here is the fine tune done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-MLMZtMbvAmyxQ8EUTFYC6yBt\",\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"created_at\": 1736327246,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-MXa32HRO92uJDJRTrzMzCZYo\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-H6QhzWgFj7Bhqyjs5CDESm\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": \"auto\",\n",
      "    \"learning_rate_multiplier\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": {},\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"seed\": 710340050,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"method\": {\n",
      "    \"type\": \"supervised\",\n",
      "    \"supervised\": {\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": \"auto\",\n",
      "        \"learning_rate_multiplier\": \"auto\",\n",
      "        \"n_epochs\": 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "\n",
    "# Upload training and validation files\n",
    "train_file = openai.File.create(file=open(\"fine_tun_json/class_prepared_train.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "valid_file = openai.File.create(file=open(\"fine_tun_json/class_prepared_valid.jsonl\", \"rb\"), purpose=\"fine-tune\")\n",
    "\n",
    "# Access the file IDs\n",
    "train_file_id = train_file['id']\n",
    "\n",
    "# Fine-tune the model with uploaded files\n",
    "fine_tune_response = openai.FineTuningJob.create(\n",
    "    training_file=train_file_id,\n",
    "    model=\"gpt-3.5-turbo\",  # Replace 'davinci' with 'gpt-3.5-turbo'\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 4  # Number of training epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print fine-tuning job details\n",
    "print(fine_tune_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-MLMZtMbvAmyxQ8EUTFYC6yBt\",\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"created_at\": 1736327246,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-MXa32HRO92uJDJRTrzMzCZYo\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"failed\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-H6QhzWgFj7Bhqyjs5CDESm\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": \"auto\",\n",
      "    \"learning_rate_multiplier\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": {\n",
      "    \"code\": \"invalid_training_file\",\n",
      "    \"param\": \"training_file\",\n",
      "    \"message\": \"The job failed due to an invalid training file. Invalid file format. Input file file-H6QhzWgFj7Bhqyjs5CDESm is in the prompt-completion format, but the specified model gpt-3.5-turbo-0125 is a chat model and requires chat-formatted data. See https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset for details.\"\n",
      "  },\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"seed\": 710340050,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"method\": {\n",
      "    \"type\": \"supervised\",\n",
      "    \"supervised\": {\n",
      "      \"hyperparameters\": {\n",
      "        \"n_epochs\": 4,\n",
      "        \"batch_size\": \"auto\",\n",
      "        \"learning_rate_multiplier\": \"auto\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "job_id = \"ftjob-MLMZtMbvAmyxQ8EUTFYC6yBt\"\n",
    "job_status = openai.FineTuningJob.retrieve(id=job_id)\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert data to the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted data saved to class_prepared_chat_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Input and output file paths\n",
    "input_file = \"fine_tun_json/class_prepared_train.jsonl\"\n",
    "output_file = \"class_prepared_chat_train.jsonl\"\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        # Parse the JSON line\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Convert to chat format\n",
    "        chat_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for product classification.\"},\n",
    "                {\"role\": \"user\", \"content\": data[\"prompt\"].replace(\"->\", \"\").strip()},\n",
    "                {\"role\": \"assistant\", \"content\": data[\"completion\"].strip()}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Write the new format to the output file\n",
    "        outfile.write(json.dumps(chat_entry) + \"\\n\")\n",
    "\n",
    "print(f\"Reformatted data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"file\",\n",
      "  \"id\": \"file-Q4XdNSuDcyXsA3aBY1anDE\",\n",
      "  \"purpose\": \"fine-tune\",\n",
      "  \"filename\": \"file\",\n",
      "  \"bytes\": 1092566,\n",
      "  \"created_at\": 1736344970,\n",
      "  \"status\": \"processed\",\n",
      "  \"status_details\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_file = openai.File.create(\n",
    "    file=open(\"class_prepared_chat_train.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\")\n",
    "print(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-elokQY9qrDOzyL4sMNI8yxAl\",\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"created_at\": 1736327697,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-MXa32HRO92uJDJRTrzMzCZYo\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-WYEKpDJoE2RtMkHvdT1vwp\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": \"auto\",\n",
      "    \"learning_rate_multiplier\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": {},\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"seed\": 1334247663,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"method\": {\n",
      "    \"type\": \"supervised\",\n",
      "    \"supervised\": {\n",
      "      \"hyperparameters\": {\n",
      "        \"batch_size\": \"auto\",\n",
      "        \"learning_rate_multiplier\": \"auto\",\n",
      "        \"n_epochs\": 4\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fine_tune_response = openai.FineTuningJob.create(\n",
    "    training_file=train_file['id'],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 4})\n",
    "print(fine_tune_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-elokQY9qrDOzyL4sMNI8yxAl\",\n",
      "  \"model\": \"gpt-3.5-turbo-0125\",\n",
      "  \"created_at\": 1736327697,\n",
      "  \"finished_at\": 1736330677,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0125:sparrow::AnNDUonF\",\n",
      "  \"organization_id\": \"org-MXa32HRO92uJDJRTrzMzCZYo\",\n",
      "  \"result_files\": [\n",
      "    \"file-HkvbVnxtVhWFg9EKG6uutE\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-WYEKpDJoE2RtMkHvdT1vwp\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": 13,\n",
      "    \"learning_rate_multiplier\": 2\n",
      "  },\n",
      "  \"trained_tokens\": 606876,\n",
      "  \"error\": {},\n",
      "  \"user_provided_suffix\": null,\n",
      "  \"seed\": 1334247663,\n",
      "  \"estimated_finish\": null,\n",
      "  \"integrations\": [],\n",
      "  \"method\": {\n",
      "    \"type\": \"supervised\",\n",
      "    \"supervised\": {\n",
      "      \"hyperparameters\": {\n",
      "        \"n_epochs\": 4,\n",
      "        \"batch_size\": 13,\n",
      "        \"learning_rate_multiplier\": 2.0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "job_id = \"ftjob-elokQY9qrDOzyL4sMNI8yxAl\"  # Replace with your job ID\n",
    "job_status = openai.FineTuningJob.retrieve(id=job_id)\n",
    "print(job_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model against an specific item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasteners 0% \n",
      "Valves, Actuator 0% \n",
      "Electrical Equipment & Supplies 0% \n",
      "Fasteners, Valves 0% \n",
      "Pipes, Fittings & Flanges 0% \n",
      "Tools & Hardware 100%\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = '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'\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:sparrow::AnNDUonF\",  # Replace with your model ID\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for product classification. Provide probabilities for each category in percentage format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"dichtungclamp\"}])\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Testing the model with reviewed Adam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mil=pd.read_pickle('df_mil_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Id</th>\n",
       "      <th>Quality Status</th>\n",
       "      <th>Manufacturer Name</th>\n",
       "      <th>Manufacturer PID</th>\n",
       "      <th>model</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Long Description</th>\n",
       "      <th>Additional description</th>\n",
       "      <th>score</th>\n",
       "      <th>Product family</th>\n",
       "      <th>...</th>\n",
       "      <th>script_decision</th>\n",
       "      <th>script_helper</th>\n",
       "      <th>mfr_helper</th>\n",
       "      <th>noun_helper</th>\n",
       "      <th>mfr noun result match</th>\n",
       "      <th>qa_group</th>\n",
       "      <th>qa_label</th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_noun</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61959169</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agitator disc Polyurethan</td>\n",
       "      <td>Agitator disc Polyurethan; ; Agitator disc Pol...</td>\n",
       "      <td>Agitator disc Polyurethan ( 2 Nos.); Machnine ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Piping Materials</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Null result present</td>\n",
       "      <td>22104400</td>\n",
       "      <td>Gas, water and sewage installation</td>\n",
       "      <td>agitator? Stirring maybe? A kind of turbine?</td>\n",
       "      <td>agitator disc</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61999316</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCH16HF07330A5C</td>\n",
       "      <td>BCH16HF07330A5C</td>\n",
       "      <td>Canesten/Servo MotorBCH16HF07330A5C</td>\n",
       "      <td>Canesten/Servo MotorBCH16HF07330A5C; BCH16HF07...</td>\n",
       "      <td>Canesten Bottel line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Gas, water and sewage installation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Null result present</td>\n",
       "      <td>27140000</td>\n",
       "      <td>Electrical Installation Materials, device</td>\n",
       "      <td>bad extraction</td>\n",
       "      <td>motor</td>\n",
       "      <td>Valves, Actuator, Fittings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Id      Quality Status Manufacturer Name Manufacturer PID  \\\n",
       "0    61959169  SPARROW_PROCESSING               NaN            00886   \n",
       "1    61999316  SPARROW_PROCESSING               NaN  BCH16HF07330A5C   \n",
       "\n",
       "             model                    Short Description  \\\n",
       "0              NaN            Agitator disc Polyurethan   \n",
       "1  BCH16HF07330A5C  Canesten/Servo MotorBCH16HF07330A5C   \n",
       "\n",
       "                                    Long Description  \\\n",
       "0  Agitator disc Polyurethan; ; Agitator disc Pol...   \n",
       "1  Canesten/Servo MotorBCH16HF07330A5C; BCH16HF07...   \n",
       "\n",
       "                              Additional description score Product family  \\\n",
       "0  Agitator disc Polyurethan ( 2 Nos.); Machnine ...   NaN            NaN   \n",
       "1                               Canesten Bottel line   NaN            NaN   \n",
       "\n",
       "   ...                     script_decision script_helper mfr_helper  \\\n",
       "0  ...                    Piping Materials         False        NaN   \n",
       "1  ...  Gas, water and sewage installation         False        NaN   \n",
       "\n",
       "   noun_helper mfr noun result match  qa_group  \\\n",
       "0        False   Null result present  22104400   \n",
       "1        False   Null result present  27140000   \n",
       "\n",
       "                                    qa_label  \\\n",
       "0         Gas, water and sewage installation   \n",
       "1  Electrical Installation Materials, device   \n",
       "\n",
       "                                         review   cleaned_noun  \\\n",
       "0  agitator? Stirring maybe? A kind of turbine?  agitator disc   \n",
       "1                                bad extraction          motor   \n",
       "\n",
       "           predicted_category  \n",
       "0                   Fasteners  \n",
       "1  Valves, Actuator, Fittings  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mil.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mil_test=df_mil[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "fine_tuned_model = \"ft:gpt-3.5-turbo-0125:sparrow::AnNDUonF\"\n",
    "inputs = df_mil['cleaned_noun'].tolist()\n",
    "actual_labels = df_mil['qa_label'].tolist()\n",
    "\n",
    "predictions = []\n",
    "# probabilities = []\n",
    "\n",
    "for text in inputs:\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=fine_tuned_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant for product classification.\"},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "            ])\n",
    "        predictions.append(response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        predictions.append(\"Error\")\n",
    "\n",
    "df_mil['predicted_category'] = predictions\n",
    "# df_mil_test['max_probability'] = probabilities\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(actual_labels, predictions)\n",
    "report = classification_report(actual_labels, predictions, zero_division=0)\n",
    "report_df = pd.DataFrame(classification_report(actual_labels, predictions, output_dict=True)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mil.to_pickle('df_mil_gpt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78419452887538"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking GPT for probabilities is not working really well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:sparrow::AnNDUonF\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for product classification.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Classify this text: 'explosionproof plug and sleeve connector'. Provide probabilities for each category in percentage format.\"}])\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mil=df_mil.drop(columns='max_probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mismatch = df_mil[df_mil['qa_label'] != df_mil['predicted_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Id</th>\n",
       "      <th>Quality Status</th>\n",
       "      <th>Manufacturer Name</th>\n",
       "      <th>Manufacturer PID</th>\n",
       "      <th>model</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Long Description</th>\n",
       "      <th>Additional description</th>\n",
       "      <th>score</th>\n",
       "      <th>Product family</th>\n",
       "      <th>...</th>\n",
       "      <th>script_decision</th>\n",
       "      <th>script_helper</th>\n",
       "      <th>mfr_helper</th>\n",
       "      <th>noun_helper</th>\n",
       "      <th>mfr noun result match</th>\n",
       "      <th>qa_group</th>\n",
       "      <th>qa_label</th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_noun</th>\n",
       "      <th>predicted_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61959169</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Agitator disc Polyurethan</td>\n",
       "      <td>Agitator disc Polyurethan; ; Agitator disc Pol...</td>\n",
       "      <td>Agitator disc Polyurethan ( 2 Nos.); Machnine ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Piping Materials</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Null result present</td>\n",
       "      <td>22104400</td>\n",
       "      <td>Gas, water and sewage installation</td>\n",
       "      <td>agitator? Stirring maybe? A kind of turbine?</td>\n",
       "      <td>agitator disc</td>\n",
       "      <td>Fasteners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61999316</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BCH16HF07330A5C</td>\n",
       "      <td>BCH16HF07330A5C</td>\n",
       "      <td>Canesten/Servo MotorBCH16HF07330A5C</td>\n",
       "      <td>Canesten/Servo MotorBCH16HF07330A5C; BCH16HF07...</td>\n",
       "      <td>Canesten Bottel line</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Gas, water and sewage installation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Null result present</td>\n",
       "      <td>27140000</td>\n",
       "      <td>Electrical Installation Materials, device</td>\n",
       "      <td>bad extraction</td>\n",
       "      <td>motor</td>\n",
       "      <td>Valves, Actuator, Fittings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61768659</td>\n",
       "      <td>SPARROW_PROCESSING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BREITNER-ABFÜLLANLAGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Motor Antriebsband</td>\n",
       "      <td>Motor Antriebsband; BREITNER-ABFÜLLANLAGE;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Gas, water and sewage installation</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Null result present</td>\n",
       "      <td>23070100</td>\n",
       "      <td>Fasteners</td>\n",
       "      <td>bad extraction</td>\n",
       "      <td>motor</td>\n",
       "      <td>Valves, Actuator, Fittings</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Id      Quality Status Manufacturer Name       Manufacturer PID  \\\n",
       "0    61959169  SPARROW_PROCESSING               NaN                  00886   \n",
       "1    61999316  SPARROW_PROCESSING               NaN        BCH16HF07330A5C   \n",
       "2    61768659  SPARROW_PROCESSING               NaN  BREITNER-ABFÜLLANLAGE   \n",
       "\n",
       "             model                    Short Description  \\\n",
       "0              NaN            Agitator disc Polyurethan   \n",
       "1  BCH16HF07330A5C  Canesten/Servo MotorBCH16HF07330A5C   \n",
       "2              NaN                   Motor Antriebsband   \n",
       "\n",
       "                                    Long Description  \\\n",
       "0  Agitator disc Polyurethan; ; Agitator disc Pol...   \n",
       "1  Canesten/Servo MotorBCH16HF07330A5C; BCH16HF07...   \n",
       "2        Motor Antriebsband; BREITNER-ABFÜLLANLAGE;    \n",
       "\n",
       "                              Additional description score Product family  \\\n",
       "0  Agitator disc Polyurethan ( 2 Nos.); Machnine ...   NaN            NaN   \n",
       "1                               Canesten Bottel line   NaN            NaN   \n",
       "2                                                NaN   NaN            NaN   \n",
       "\n",
       "   ...                     script_decision script_helper mfr_helper  \\\n",
       "0  ...                    Piping Materials         False        NaN   \n",
       "1  ...  Gas, water and sewage installation         False        NaN   \n",
       "2  ...  Gas, water and sewage installation         False        NaN   \n",
       "\n",
       "   noun_helper mfr noun result match  qa_group  \\\n",
       "0        False   Null result present  22104400   \n",
       "1        False   Null result present  27140000   \n",
       "2        False   Null result present  23070100   \n",
       "\n",
       "                                    qa_label  \\\n",
       "0         Gas, water and sewage installation   \n",
       "1  Electrical Installation Materials, device   \n",
       "2                                  Fasteners   \n",
       "\n",
       "                                         review   cleaned_noun  \\\n",
       "0  agitator? Stirring maybe? A kind of turbine?  agitator disc   \n",
       "1                                bad extraction          motor   \n",
       "2                                bad extraction          motor   \n",
       "\n",
       "           predicted_category  \n",
       "0                   Fasteners  \n",
       "1  Valves, Actuator, Fittings  \n",
       "2  Valves, Actuator, Fittings  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mismatch.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Model from Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) Retraining all model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bert model not working right now, in embeddings.ipynb similar thing working for a smaller model trained in many languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e9a258f92f40f3ac575471ee20c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70f80a57a034e7baf475a850351a54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4d8a47c5e94612a4033e1ee0b31342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1850bd5eac4d308a83064e22068958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2d44c967cc4f3ab81ee8ca7fb54b13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9951 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 63\u001b[0m\n\u001b[0;32m     54\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     55\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m     56\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     66\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2165\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2166\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2167\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2168\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2169\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2524\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2517\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2518\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2519\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2520\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2522\u001b[0m )\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2524\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2527\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2530\u001b[0m ):\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3687\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3685\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3687\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3688\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RodrigoMartínezAlons\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(classify, test_size=0.2)\n",
    "train_dataset = Dataset.from_pandas(train_df[['cleaned_noun', 'category_noun']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['cleaned_noun', 'category_noun']])\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_noun\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_dataset['category_noun'])\n",
    "\n",
    "def encode_labels(batch):\n",
    "    batch['label'] = label_encoder.transform(batch['category_noun'])\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels, batched=True)\n",
    "val_dataset = val_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    save_strategy=\"epoch\", \n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True  \n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'eval_accuracy': accuracy}\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) PEFT, using Lora to freeze some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c2b50e19ad4a70b547fee1c6b60126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bdb833a67f4ce5b7ab3e3a7efab2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf321c5377f42c58d66b432cbbb902e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff12fb44b8240cabc2c022ee75fbedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_noun\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the category_noun column from the train and validation datasets\n",
    "label_encoder.fit(train_dataset['category_noun'])\n",
    "\n",
    "# Function to apply the label transformation\n",
    "def encode_labels(batch):\n",
    "    # Map the 'category_noun' column to encoded labels\n",
    "    batch['label'] = label_encoder.transform(batch['category_noun'])\n",
    "    return batch\n",
    "\n",
    "# Apply the label encoding function to both the train and validation datasets\n",
    "train_dataset = train_dataset.map(encode_labels, batched=True)\n",
    "val_dataset = val_dataset.map(encode_labels, batched=True)\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7)\n",
    "\n",
    "# Initialize LoRA using PEFT (Ensure correct LoRA configuration)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank (adjustable, controls how much we adapt the model)\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout in the LoRA layers\n",
    "    target_modules=[\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"]  # Adjust the layers if needed\n",
    ")\n",
    "\n",
    "# Fine-tuning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", \n",
    "    num_train_epochs=3, \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",  # Evaluates the model after each epoch\n",
    "    save_strategy=\"epoch\",  # Saves the model after each epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True  # Load the best model after training\n",
    ")\n",
    "\n",
    "# Define the compute_metrics function for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Get the most likely predictions\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"eval_accuracy\": accuracy}\n",
    "\n",
    "# Create the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Evaluation results: {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
